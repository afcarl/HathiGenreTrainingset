LabNotebook

vocabularies, sorted in order of size, where we also print the things that get *lost* rather than added each time we move up a size category. I’ve marked the ones I consider best.

reducedvocabulary2.txt    ****
500
set()
----------------
enlargedvocabulary.txt   ****
641
{'op', 'money', 'z', 'io'}
----------------
enlargedvocabulary copy.txt
648
{'appendix', 'preface'}
----------------
thousandvocabulary.txt
880
{'lb', 'woodcuts', 'edges', 'pp', 'le', 'trans', 'cents', 'rh', 'honourable', 'siege', 'ab', 'port', 'ai', 'fcp', 'wines', 'heat', 'ditto', 'net', 'caesar', 'irish', 'eo', 'fort', 'don', 'seq', 'ant', 'oo', 'os', 'preached', 'sc', 'wine', 'п', 'archbishop', 'ode', 'december', 'dictionary', 'je', 'sermon'}
----------------
newmethodvocabulary.txt  ****
935
{'ç', 'ens', 'c', 'pro', 'í', 'а', 'ger', 'lie', 'ae', 'т', 'com', 'cut', 'imp', 'io', 'low', 'adj', 'opinion', 'г', 'adv', 'strong', 'clerk', 'с', 'um', 'performed', 'fifth', 'par'}
----------------
biggestvocabulary.txt   ****
1036
set()
----------------
reallybiggestvocabulary.txt
1307
set()
----------------
mergedvocabulary.txt
1328
set()
----------------

newfeatures - newfeatures4:

Gradually removing structural features, changing ways of calculating the influence of call numbers

newfeatures5 -- I removed call numbers altogether. Also removed some superfluous two-letter words in the vocabulary.

newfeatures6 -- I removed other forms of metadata.

newfeatures7 -- Put back the metadata features for fiction and biography. Seemed to help.

newfeatures8 -- Took out periods, quotation marks, and endwpunct.

newfeatures9 -- Used reducedvocabulary, 460-odd features. Ridge parameter 10. 9 folds.

bydate -- reducedvocab2, 498 features. Ridge parameter 10. 9 folds.

bycallno -- reducedvocab2, 499 features. Ridge parameter 32. 10 folds. Only one round of smoothing.

svm - self-explanatory

forest - first random forest model; best so far .937; reduced vocab, no call # features; one-vs-all

forest2 - believe I may have enlarged the vocab - doubled the smoothing - made the model weaker 

forest3 -- reduced vocab but added in call # features -- took out multiple smoothing -- didn’t suck .927

forest4 - first random forest model that used multiclass directly -- still had call # features and reduced vocab .921. So far all models have had 100 or 110 trees, K 20 or 22.

forest5 - Multiclass, 200 trees, 16 features each, enlarged (640) vocab, no call no info, single smoothing. 6fold. Decent results: .9322

forest6 - Multiclass, 200 trees, 25 features each, enlarged (640) vocab, no call no info, single smoothing, added back (Math.abs(sumAllWords - meanWordsPerPage) / meanWordsPerPage) and endwpunct / textlines. Still 6fold. Better results: .9345

forest7 - Pretty much as above, except 300 trees and 30 features. Switched to Weka 3.7.11 also. Rough .8896 smoothed .9406.

forest8 - With Weka 3.7, RandomForests get parallelized, so let’s kick this up a notch. Larger vocab of 879 words; 1000 trees; 35 features per tree. .9344

forest9 - Larger vocab; 500 trees, 50 features per tree. We also kicked out a couple of items from the training set that aren’t in English. .9415

forest10 - Getting rid of more items from training set. Also added back in the “quotations” feature, which really appears not to help. .9402

forest11 -- Took back out the quotations. Increased number of features per tree from 50 to 80. Did not help .93705

forest11b -- because I forgot to create 12 for it -- Took it back down to 32 features. Smoothed, .9343, coalesced .9400. I’m getting a sense that the sweet spot for features-per-tree is between 30 and 50.

forest12 -- Different underlying dataset (switched to thirdfeatures). This means adding stdev of line lengths, as well as textline/totalline ratios, and upvoting header words. Kept everything else the same. Smoothed .9353, coalesced .9370. For some reason coalescing not helping much on this run.

forest13 -- Kept everything the same but killed the stdev feature. Smoothed .9350, coalesced .9418. Starting to think that it’s dangerous to judge solely on coalesced, as I’ve been doing, because it has a fairly unpredictable relation to underlying classification accuracy.

forest14 -- Yep. Coalesced is unreliable. Put back in *normalized* stdev, increased features to 40 (from 32).  Smoothed .9361, coalesced .9385. Actually that does seem to show that something about stdev is huring coalesce.

forest15 -- Killed stdev again. Kept 40 features. Switched from thousandvocab to enlargedvocab, now with “preface” and “appendix” added as features. Smoothed .9360, coalesced .9404. Could be better with lower features, in proportion to smaller vocab. But now we’re quibbling about details.

newlog0 -- switched back to logistic regression, with same vocab, ridge 32, still 6fold: smoothed .9276, coalesced .9377

forest16 -- Switched from multiclass single forest to multiple runs of different one-vs-all forests. 22 features per tree, 642 features overall, 500 trees in each forest. Smoothed .9328. Coalesced .9375.

production0 - I tried separating “index.” No success; even with multiclassforest, it brought accuracy down 1%.

production1 -- Back to larger vocab, 32 features, 5fold crossvalidation, smoothed .9343, coalesced .9406. Okay. I’m actually ready to roll.

production2 = Kept everything the same. 881 vocab. 32 features. -train -multiclassforest -troot /Users/tunder/Dropbox/pagedata/ -tbranch thirdfeatures -self -save -output /Volumes/TARDIS/output/production2/ -bio
Didn’t work, because I had only implemented multiclassforest under crossvalidate. But trained a logistic model we can use, with 881 vocab and ridge 32.

production3 = Second run with previous settings.

production4= Tried outputjson, reducedvocabulary2; 20 features per tree.

production5 = ensemble results
-ensemble /Volumes/TARDIS/input/ensemble/ -local -toprocess /Users/tunder/Dropbox/pagedata/thirdfeatures/pagefeatures/ -output /Volumes/TARDIS/output/production5/ 

production 6 = Multiclassforest; reducedvocab2; 499 features; 20 features per tree.

production7 -- An ensemble produced with /Volumes/TARDIS/input/ensemble/production2logistic881.ser	logistic881	-onevsalllogistic
/Volumes/TARDIS/input/ensemble/production3forest881.ser	forest881	-multiclassforest
/Volumes/TARDIS/input/ensemble/production6forest500.ser	forest500	-multiclassforest

forest17 -- 8fold crossvalidation of the model trained in production6. .9357/.9436

newlog1 - 8fold crossvalidation of the model trained in production2.

forest18 - 8fold crossvalidation of a slight variation on production3. newmethodvocabulary, 35 features per tree. .9358 / .9422

newlog2 - 8fold, logistic, newmethodvocab, 936 features, ridge setting at “3.” 92.7 smoothed, 94.5 coalesced. It really kicks up the performance of the ensemble, from .9337/.942 to .9344/.9496. Notice that almost all this improvement is after coalescing, which suggests that the reduction of ridge parameter may be overfitting on individual pages in a way that nevertheless helps macroscopic performance.

post1799 -- Partial model trained only on 19c texts. 8fold random forest, enlarged vocab (642 features), 27 features per tree, 500 trees.
pre1850 -- Partial model trained only on 19c texts. 8fold random forest, enlarged vocab (642 features), 27 features per tree, 500 trees.

These datasets are appx equal in size (about 200 vols). Pre1850 oddly does better than post 1799. Neither does great. When they are merged (dividing the set at 1820), we get lackluster results -- smoothed .916 coalesced .918.  Folding that model into the ensemble hurts the ensemble.

Currently, good ensemble is: listofmodels = ["newlog1", "forest18", "forest17", "newlog2"]
.9345 / .9504

Trying: newlog3 -- reducedvocab2 (500 features), ridge setting 6, 8fold logistic. Smoothed .9145, coalesced .9134. Somewhat to my surprise, getting better results with more features on logistic. Might try even more! 

Redid newlog3 with 1036 features. Kept ridge setting 6, 8fold logistic. Smoothed .927, coalesced .9383.

newlog4 -- 1036 features, ridge parameter “1”, 8fold logistic. .9297 .9425

listofmodels = ["newlog4", "forest18", "forest17", "newlog2"]
.9342 / .9497

forest19 - 1308 features; 38 features per tree; .9351 / .9403. Not enough improvement on forest 18, but perhaps I need more features per tree.

newlog5 - 1308 features, 8fold logistic, ridge setting “2.” Smoothed .9247, coalesced .9429

listofmodels = ["newlog1", "forest18", "forest17", "newlog2", "newlog5", "forest19"]
smoothed .9333 coalesced .9478

*********

Okay, we’re going to create an ensemble based on this:
listofmodels = ["newlog1", "forest18", "forest17", "newlog2"] .9345 / .9504

That will require four models:

production2 == newlog1. 881 vocab (thousandvocabulary), ridge setting 32.

production9 == forest18; newmethodvocabulary (936 features); 35 features per tree.

production6 == forest 17. Multiclassforest; reducedvocab2; 499 features; 20 features per tree.

production8 == newlog2. 936 features (newmethodvocab), ridge setting 3.
-train -troot /Users/tunder/Dropbox/pagedata/ -tbranch thirdfeatures -self -save -output /Volumes/TARDIS/output/production8/ -bio


