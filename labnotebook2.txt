LabNotebook

newfeatures - newfeatures4:

Gradually removing structural features, changing ways of calculating the influence of call numbers

newfeatures5 -- I removed call numbers altogether. Also removed some superfluous two-letter words in the vocabulary.

newfeatures6 -- I removed other forms of metadata.

newfeatures7 -- Put back the metadata features for fiction and biography. Seemed to help.

newfeatures8 -- Took out periods, quotation marks, and endwpunct.

newfeatures9 -- Used reducedvocabulary, 460-odd features. Ridge parameter 10. 9 folds.

bydate -- reducedvocab2, 498 features. Ridge parameter 10. 9 folds.

bycallno -- reducedvocab2, 499 features. Ridge parameter 32. 10 folds. Only one round of smoothing.

svm - self-explanatory

forest - first random forest model; best so far .937; reduced vocab, no call # features; one-vs-all

forest2 - believe I may have enlarged the vocab - doubled the smoothing - made the model weaker 

forest3 -- reduced vocab but added in call # features -- took out multiple smoothing -- didn’t suck .927

forest4 - first random forest model that used multiclass directly -- still had call # features and reduced vocab .921. So far all models have had 100 or 110 trees, K 20 or 22.

forest5 - Multiclass, 200 trees, 16 features each, enlarged (640) vocab, no call no info, single smoothing. 6fold. Decent results: .9322

forest6 - Multiclass, 200 trees, 25 features each, enlarged (640) vocab, no call no info, single smoothing, added back (Math.abs(sumAllWords - meanWordsPerPage) / meanWordsPerPage) and endwpunct / textlines. Still 6fold. Better results: .9345

forest7 - Pretty much as above, except 300 trees and 30 features. Switched to Weka 3.7.11 also. Rough .8896 smoothed .9406.

forest8 - With Weka 3.7, RandomForests get parallelized, so let’s kick this up a notch. Larger vocab of 879 words; 1000 trees; 35 features per tree. .9344

forest9 - Larger vocab; 500 trees, 50 features per tree. We also kicked out a couple of items from the training set that aren’t in English. .9415

forest10 - Getting rid of more items from training set. Also added back in the “quotations” feature, which really appears not to help. .9402

forest11 -- Took back out the quotations. Increased number of features per tree from 50 to 80. Did not help .93705

forest11b -- because I forgot to create 12 for it -- Took it back down to 32 features. Smoothed, .9343, coalesced .9400. I’m getting a sense that the sweet spot for features-per-tree is between 30 and 50.

forest12 -- Different underlying dataset (switched to thirdfeatures). This means adding stdev of line lengths, as well as textline/totalline ratios, and upvoting header words. Kept everything else the same. Smoothed .9353, coalesced .9370. For some reason coalescing not helping much on this run.

forest13 -- Kept everything the same but killed the stdev feature. Smoothed .9350, coalesced .9418. Starting to think that it’s dangerous to judge solely on coalesced, as I’ve been doing, because it has a fairly unpredictable relation to underlying classification accuracy.

forest14 -- Yep. Coalesced is unreliable. Put back in *normalized* stdev, increased features to 40 (from 32).  Smoothed .9361, coalesced .9385. Actually that does seem to show that something about stdev is huring coalesce.

forest15 -- Killed stdev again. Kept 40 features. Switched from thousandvocab to enlargedvocab, now with “preface” and “appendix” added as features. Smoothed .9360, coalesced .9404. Could be better with lower features, in proportion to smaller vocab. But now we’re quibbling about details.

newlog0 -- switched back to logistic regression, with same vocab, ridge 32, still 6fold: smoothed .9276, coalesced .9377

forest16 -- Switched from multiclass single forest to multiple runs of different one-vs-all forests. 22 features per tree, 642 features overall, 500 trees in each forest. Smoothed .9328. Coalesced .9375.

production0 - I tried separating “index.” No success; even with multiclassforest, it brought accuracy down 1%.

production1 -- Back to larger vocab, 32 features, 5fold crossvalidation, smoothed .9343, coalesced .9406. Okay. I’m actually ready to roll.

production2 = Kept everything the same. 881 vocab. 32 features. -train -multiclassforest -troot /Users/tunder/Dropbox/pagedata/ -tbranch thirdfeatures -self -save -output /Volumes/TARDIS/output/production2/ -bio
Didn’t work, because I had only implemented multiclassforest under crossvalidate. But trained a logistic model we can use.

production3 = Second run with previous settings.
